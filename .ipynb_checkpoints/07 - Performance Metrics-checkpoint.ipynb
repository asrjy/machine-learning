{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>A.</b></font> Compute performance metrics for the given data <strong>5_a.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> \n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_mat(df):\n",
    "    \"\"\"\n",
    "    This function takes the dataset as the input and returns a 2x2. \n",
    "    True Negative  - [0,0]\n",
    "    False Negative - [0,1]\n",
    "    False Positive - [1,0]\n",
    "    True Positive  - [1,1]\n",
    "    \"\"\"\n",
    "    # Initializing an empty array\n",
    "    conf_mat = [[0,0],[0,0]]\n",
    "    # Iterative over the values of dataset and filling in the values based on the conditions\n",
    "    for i in range(len(df)):\n",
    "        if(df[i][0] == 1 and df[i][2] == 1):\n",
    "            conf_mat[1][1] += 1\n",
    "        elif(df[i][0] == 1 and df[i][2] == 0):\n",
    "            conf_mat[0][1] += 1\n",
    "        elif(df[i][0] == 0 and df[i][2] == 1):\n",
    "            conf_mat[1][0] += 1\n",
    "        elif(df[i][0] == 0 and df[i][2] == 0):\n",
    "            conf_mat[0][0] += 1\n",
    "    return conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(df, tot_neg, tot_pos, thresholds):\n",
    "    \"\"\"\n",
    "    This function takes the dataset, total number of negative points, total number of positive points and the unique\n",
    "    threshold values as an input and returns the Area Under Curve Score of the dataset. \n",
    "    \"\"\"\n",
    "    # Creating two empty arrays to store the Total Positive Rate(TPR) and the False Positive Rate(FPR)\n",
    "    tpr, fpr = [],[]\n",
    "    # Iterating over the unique threshold values\n",
    "    for threshold in thresholds:\n",
    "        # Counting the number of false positives and true positives.\n",
    "        # Initializing them to 0 each iteration\n",
    "        fp_temp, tp_temp = 0,0\n",
    "        # Iterating over the whole dataset\n",
    "        for i in range(len(df)):\n",
    "            # If the probability is >= threshold i.e., Model predicts 1\n",
    "            if(df[i][1] >= threshold):\n",
    "                # If the actual value is 1, ie., It's a True Positive. Incrementing it's value\n",
    "                if(df[i][0] == 1):\n",
    "                    tp_temp += 1\n",
    "                # Else if the actual value is 0, i.e., It's a False Positive. Incrementing it's value\n",
    "                elif(df[i][0] == 0):\n",
    "                    fp_temp += 1\n",
    "            # We don't need the False Negatives and True Negatives. \n",
    "            # If the probability is < threshold., we can just break and exit the for loop. \n",
    "            # Before breaking, calculating the FPR and TPR values and appending them to the list. \n",
    "            elif(df[i][1] < threshold):\n",
    "                fpr.append(fp_temp/tot_neg)\n",
    "                tpr.append(tp_temp/tot_pos)\n",
    "                break\n",
    "    # Calculating the AUC Score using the TPR and FPR lists. \n",
    "    # Before calculating, reversing the lists as the default order returns negative AUC Score\n",
    "    fpr = fpr[::-1]\n",
    "    tpr = tpr[::-1]\n",
    "    return np.trapz(tpr, fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_threshold(df, thresholds):\n",
    "    \"\"\"\n",
    "    This function takes the dataset and returns the threshold that gives the lowest metric.\n",
    "    \"\"\"\n",
    "    # lowest_a will store the lowest_a value calculated so far. Initially it's going to be -1\n",
    "    lowest_a = -1\n",
    "    # best_threshold will store the threshold value that produced the lowest_a\n",
    "    best_threshold = 0\n",
    "    # Iterative over all the thresholds\n",
    "    for threshold in thresholds:\n",
    "        # Storing the values of the confusion matrix at each iteration\n",
    "        # a_temp stores the current A value\n",
    "        fp_temp, tp_temp, fn_temp, tn_temp, a_temp = 0,0,0,0,0\n",
    "        # Iterating over the dataset\n",
    "        for i in range(len(df)):\n",
    "            # If the probability is >= threshold ie., Model predicts 1\n",
    "            if(df[i][1] >= threshold):\n",
    "                # If the actual value is also 1, incrementing the value of True Positive\n",
    "                if(df[i][0] == 1):\n",
    "                    tp_temp += 1\n",
    "                # If the actual value is 0, incrementing the value of False Positive\n",
    "                elif(df[i][0] == 0):\n",
    "                    fp_temp += 1\n",
    "            # If the probability is < threshold ie., Model predicts 0\n",
    "            elif(df[i][1] < threshold):\n",
    "                # If the actual value is 1, incrementing the value of False Negative\n",
    "                if(df[i][0] == 1):\n",
    "                    fn_temp += 1\n",
    "                # If the actual value is alse 0, incrementing the value of True Negative\n",
    "                elif(df[i][0] == 0):\n",
    "                    tn_temp += 1\n",
    "        # Calculating the value of a\n",
    "        a_temp = (500*fn_temp)+(100*fp_temp)\n",
    "        # If the value of a is -1 ie., this is the first iteration\n",
    "        if(lowest_a == -1):\n",
    "            # Storing the value of a in lowest_a\n",
    "            lowest_a = a_temp\n",
    "        # Else if the value of a is less than lowest_a\n",
    "        elif(a_temp <= lowest_a):\n",
    "            # Storing the value of a in lowest_a\n",
    "            lowest_a = a_temp\n",
    "            # Storing the threshold in best_threshold\n",
    "            best_threshold = threshold\n",
    "    # Returning the best_threshold\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [],
   "source": [
    "# Importing the dataset using numpy\n",
    "df_5a = np.genfromtxt('./Datasets/5_a.csv', delimiter=',')\n",
    "# Removing the first row of the dataset (names of columns)\n",
    "df_5a = np.delete(df_5a, 0, 0)\n",
    "# Making forecasts based on the probability scores and storing them in a preds array\n",
    "preds_5a = np.array([1 if df_5a[i][1] >= 0.5 else 0 for i in range(len(df_5a))])\n",
    "# Adding the predictions array to the dataset as a new column\n",
    "df_5a = np.c_[df_5a, preds_5a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Confusion Matrix is [[0, 0], [100, 10000]]\n",
      "The F1 Score is 0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "# Using the get_conf_mat to get the Confusion Matrix\n",
    "conf_mat_5a = get_conf_mat(df_5a)\n",
    "# Calculating the Precision by dividing the True Positive by sum of True and False positives\n",
    "precision_5a = conf_mat_5a[1][1]/(conf_mat_5a[1][0]+conf_mat_5a[1][1])\n",
    "# Calculating the Recall by dividing the True Positive by sum of False Negative and True Positive\n",
    "recall_5a = conf_mat_5a[1][1]/(conf_mat_5a[0][1] + conf_mat_5a[1][1])\n",
    "# Calculating the F1 score by taking the Harmonic Mean of Precision and Recall\n",
    "f1_5a = 2*(precision_5a*recall_5a)/(precision_5a+recall_5a)\n",
    "# Printing the F1 scores and the confusion Matrix\n",
    "print(f\"The Confusion Matrix is {conf_mat_5a}\")\n",
    "print(f\"The F1 Score is {f1_5a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dataset by the probabilites \n",
    "df_5a_sorted = df_5a[df_5a[:,1].argsort()][::-1]\n",
    "# Getting the unique probabilites and storing them in a thresholds list\n",
    "thresholds_5a = np.unique(df_5a_sorted[:,1])\n",
    "# Getting the total number of Positive and Negative points by summing up the columns of the Confusion Matrix\n",
    "tot_neg_5a, tot_pos_5a  = conf_mat_5a[0][0]+conf_mat_5a[1][0], conf_mat_5a[0][1]+conf_mat_5a[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC Score is 0.48829900000000004\n"
     ]
    }
   ],
   "source": [
    "# Getting the AUC Score using the get_auc function\n",
    "auc_5a = get_auc(df_5a_sorted, tot_neg_5a, tot_pos_5a, thresholds_5a)\n",
    "print(f\"The AUC Score is {auc_5a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the dataset is 0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Accuracy \n",
    "accuracy_5a = (conf_mat_5a[0][0]+conf_mat_5a[1][1])/(conf_mat_5a[0][0]+conf_mat_5a[0][1]+conf_mat_5a[1][0]+conf_mat_5a[1][1])\n",
    "print(f\"The accuracy of the dataset is {accuracy_5a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>B.</b></font> Compute performance metrics for the given data <strong>5_b.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> \n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a></li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [],
   "source": [
    "# Importing the dataset using numpy\n",
    "df_5b = np.genfromtxt('./Datasets/5_b.csv', delimiter=',')\n",
    "# Removing the first row of the dataset (names of columns)\n",
    "df_5b = np.delete(df_5b, 0, 0)\n",
    "# Making forecasts based on the probability scores and storing them in a preds array\n",
    "preds_5b = np.array([1 if df_5b[i][1] >= 0.5 else 0 for i in range(len(df_5b))])\n",
    "# Adding the predictions array to the dataset as a new column\n",
    "df_5b = np.c_[df_5b, preds_5b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Confusion Matrix is [[9761, 45], [239, 55]]\n",
      "The F1 Score is 0.2791878172588833\n"
     ]
    }
   ],
   "source": [
    "# Using the get_conf_mat to get the Confusion Matrix\n",
    "conf_mat_5b = get_conf_mat(df_5b)\n",
    "# Calculating the Precision by dividing the True Positive by sum of True and False positives\n",
    "precision_5b = conf_mat_5b[1][1]/(conf_mat_5b[1][0]+conf_mat_5b[1][1])\n",
    "# Calculating the Recall by dividing the True Positive by sum of False Negative and True Positive\n",
    "recall_5b = conf_mat_5b[1][1]/(conf_mat_5b[0][1] + conf_mat_5b[1][1])\n",
    "# Calculating the F1 score by taking the Harmonic Mean of Precision and Recall\n",
    "f1_5b = 2*(precision_5b*recall_5b)/(precision_5b+recall_5b)\n",
    "# Printing the F1 scores and the confusion Matrix\n",
    "print(f\"The Confusion Matrix is {conf_mat_5b}\")\n",
    "print(f\"The F1 Score is {f1_5b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dataset by the probabilites \n",
    "df_5b_sorted = df_5b[df_5b[:,1].argsort()][::-1]\n",
    "# Getting the unique probabilites and storing them in a thresholds list\n",
    "thresholds_5b = np.unique(df_5b_sorted[:,1])\n",
    "# Getting the total number of Positive and Negative points by summing up the columns of the Confusion Matrix\n",
    "tot_neg_5b, tot_pos_5b  = conf_mat_5b[0][0]+conf_mat_5b[1][0], conf_mat_5b[0][1]+conf_mat_5b[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC Score is 0.9376570000000001\n"
     ]
    }
   ],
   "source": [
    "# Getting the AUC Score using the get_auc function\n",
    "auc_5b = get_auc(df_5b_sorted, tot_neg_5b, tot_pos_5b, thresholds_5b)\n",
    "print(f\"The AUC Score is {auc_5b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the dataset is 0.9718811881188119\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Accuracy \n",
    "accuracy_5b = (conf_mat_5b[0][0]+conf_mat_5b[1][1])/(conf_mat_5b[0][0]+conf_mat_5b[0][1]+conf_mat_5b[1][0]+conf_mat_5b[1][1])\n",
    "print(f\"The accuracy of the dataset is {accuracy_5b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "<font color='red'><b>C.</b></font> Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data <strong>5_c.csv</strong>\n",
    "<br>\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [],
   "source": [
    "# Importing the dataset using numpy\n",
    "df_5c = np.genfromtxt('./Datasets/5_c.csv', delimiter=',')\n",
    "# Removing the first row of the dataset (names of columns)\n",
    "df_5c = np.delete(df_5c, 0, 0)\n",
    "# Making forecasts based on the probability scores and storing them in a preds array\n",
    "preds_5c = np.array([1 if df_5c[i][1] >= 0.5 else 0 for i in range(len(df_5c))])\n",
    "# Adding the predictions array to the dataset as a new column\n",
    "df_5c = np.c_[df_5c, preds_5c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the get_conf_mat to get the Confusion Matrix\n",
    "conf_mat_5c = get_conf_mat(df_5c)\n",
    "# Calculating the Precision by dividing the True Positive by sum of True and False positives\n",
    "df_5c_sorted = df_5c[df_5c[:,1].argsort()][::-1]\n",
    "# Calculating the Recall by dividing the True Positive by sum of False Negative and True Positive\n",
    "thresholds_5c = np.unique(df_5c_sorted[:,1])\n",
    "# Calculating the F1 score by taking the Harmonic Mean of Precision and Recall\n",
    "tot_neg_5c, tot_pos_5c  = conf_mat_5c[0][0]+conf_mat_5c[1][0], conf_mat_5c[0][1]+conf_mat_5c[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the get_best_threshold to get the best threshold value resposnsible for lowest A\n",
    "best_t_5c = get_best_threshold(df_5c_sorted, thresholds_5c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2300390278970873"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_t_5c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>D.</b></font> Compute performance metrics(for regression) for the given data <strong>5_d.csv</strong>\n",
    "    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset using numpy\n",
    "df_5d = np.genfromtxt('./Datasets/5_d.csv', delimiter=',')\n",
    "# Removing the first row of the dataset (names of columns)\n",
    "df_5d = np.delete(df_5d, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the mean of actual values in mean_y\n",
    "mean_y = np.mean(df_5d[:,0])\n",
    "# Initializing the values of ss_total, ss_residual and MAPE to 0\n",
    "ss_total, ss_residual, mape, mse = 0, 0, 0, 0\n",
    "# Iterating over the dataset\n",
    "for i in range(len(df_5d)):\n",
    "    # Calculating the ss_total, ss_residual and mape numerator\n",
    "    ss_total += (df_5d[i][0] - mean_y)**2\n",
    "    ss_residual += (df_5d[i][0] - df_5d[i][1])**2\n",
    "    mape += np.abs(df_5d[i][0] - df_5d[i][1])\n",
    "# Calculating the MSE, MAPE and R^2 values\n",
    "mse = ss_residual/len(df_5d)\n",
    "mape /= np.sum(df_5d[:,0])\n",
    "r_squared = 1-(ss_residual/ss_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. MSE : 177.16569974554707\n",
      "2. MAPE : 0.1291202994009687\n",
      "3. R^2 : 0.9563582786990964\n"
     ]
    }
   ],
   "source": [
    "# Printing obtained values\n",
    "print(f\"1. MSE : {mse}\\n2. MAPE : {mape}\\n3. R^2 : {r_squared}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
